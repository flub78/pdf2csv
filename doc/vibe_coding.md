# Vibe coding

* Globally GitHub Copilot is able to generate working code.

* Global feeling is that it is slow for easy (for a human) requests but relatively fast for semi-complex requests. Sometimes really slow even for simple prompts. I assume that it was rather a server workload issue than anything else. It is quite impressive and correct to generate a project structure.

* It is not able to handle complex requests, the global request to generate a converter from an input and output example failed. It was easy to check that the generated output was incorrect, but it provided a wrong answer anyway, even if it often generates tests for its code.

* By mistake I made a request related to this project in a totally different one. The AI tried to do something even if it was obvious that the project had nothing to do with the request.

* It does not learn from previous prompts. When working on a limited portion of code on a project, a human learns and becomes more and more efficient. The AI not. Code analysis to find where to change the code is done over and over. For example, if you ask AI to change the color of a button, it has to analyze the code to find where it is handled. If you change your mind and ask for a different color, the analysis is done again. 

* The quality of the code is rather poor. It is not really bad code, but obvious optimizations are often missed. It is important to keep the code base minimal for a feature or a project. Maintenance costs are often proportional to the size of the code (or can even grow faster than the size of the code). When not explicitly requested, the AI generates the same code again and again and often misses the fact that a function with parameters could be used. In the current state of the art, it could quickly become an issue for medium and big projects. It would need more investigation but I would not be surprised by projects becoming two or three times bigger than the same ones controlled by human beings.

* And may be the worst part, the human in charge does not really learn the code. After a while working on a project, you know it well and become rather efficient. With AI, you tend to generate legacy code. What I mean by that is that the human in charge becomes not more familiar with the code that he is in front of a project created and maintained by someone else. If AI do not make sufficient progress to be trusted, you get a team with a not so smart AI and a human not really in control of the code. it could quickly get out of control.

* Sometimes incredibly helpful, for example it is able to generate synthetic documents to have for each statement operation, the original test to parse, the official correct answer provided by the bank and the output generated by the parser. This document has been generated in a few seconds and would have taken hours to be created manually. It has also been able to make a complete review of all regular expressions used in the project for analysis. The manual generation of such document would have taken hours.

* Temporary conclusion: AI is currently a goodwill assistant rather dumb, often useful and helping but requesting constant supervision. It cannot be trusted even on simple tasks.

Il will require a lot of adaptation from the developers to be able to use AI when it is helpful, but to keep control of the whole process. 